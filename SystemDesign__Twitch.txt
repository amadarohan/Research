System Design for: 'Twitch'

This document outlines the architectural design for a scalable, real-time live-streaming platform like Twitch. The system is designed to handle a massive number of concurrent live video streams, a high volume of viewers, real-time chat, and a large-scale, low-latency content delivery system, ensuring high availability and a quality user experience.

### C4 Model Diagrams

#### 1\. System Context Diagram

The System Context diagram provides a high-level view of the Twitch system and its interactions with external users and systems. It shows the core platform interacting with streamers, viewers, and external services for game data and payments.

**Description:**

  * **Streamer:** A person who broadcasts live video content, typically of video games, to an audience.
  * **Viewer:** A person who watches live or pre-recorded streams, interacts with chat, and subscribes to channels.
  * **Twitch System (System in Scope):** The central platform that facilitates all live video streaming, content delivery, and viewer interaction.
  * **External Game Data Service:** A third-party system that provides information about games being played, such as metadata and game titles.
  * **Third-Party Payment Gateway:** An external service that securely processes payments for subscriptions, donations, and bits.

**Mermaid.js Code:**

```mermaid
C4Context
    title System Context Diagram for Twitch

    Person(streamer, "Streamer", "Broadcasts live video content.")
    Person(viewer, "Viewer", "Watches streams and interacts with chat.")
    
    System(twitch_system, "Twitch", "The core system for live-streaming.")
    
    System_Ext(game_data_service, "External Game Data Service", "Provides metadata about video games.")
    System_Ext(payment_gateway, "Third-Party Payment Gateway", "Processes payments for subscriptions and donations.")

    Rel(streamer, twitch_system, "Broadcasts video stream to")
    Rel(viewer, twitch_system, "Watches live/VOD content from")
    Rel(twitch_system, game_data_service, "Queries game info from")
    Rel(twitch_system, payment_gateway, "Processes payments via")
```

-----

#### 2\. Container Diagram

This diagram zooms into the Twitch system, showing the major independently deployable components. The design is a distributed, event-driven architecture with specialized services for media ingestion, processing, and delivery.

**Description:**

  * **Client Application (Desktop/Mobile/Web):** The user-facing application used by streamers to broadcast and by viewers to watch streams.
  * **API Gateway:** The single entry point for all client requests, handling authentication and routing to backend services.
  * **Ingestion Service:** A high-throughput service that accepts live video streams from streamers. It is designed to be geographically distributed to minimize latency.
  * **Transcoding Service:** A powerful, distributed service that transcodes incoming video streams into multiple resolutions and bitrates to support various viewer devices and network conditions.
  * **CDN (Content Delivery Network) Service:** A global network of edge servers that cache and deliver live-stream fragments to viewers with minimal latency.
  * **Stream Service:** Manages the metadata for a live stream, such as stream key, title, and game category.
  * **Chat Service:** A high-volume, real-time service that handles all in-stream chat messages.
  * **User Service:** Manages user profiles, authentication, and follower/subscription data.
  * **Video-on-Demand (VOD) Service:** Handles the storage and playback of recorded live streams.
  * **Video Blob Store:** A highly scalable object storage system (e.g., AWS S3) for storing recorded VOD content.
  * **Message Broker:** A distributed message queue (e.g., Kafka) that decouples services and handles the high-volume event stream of live data.

**Mermaid.js Code:**

```mermaid
C4Container
    title Container Diagram for Twitch

    Person(streamer, "Streamer")
    Person(viewer, "Viewer")
    
    System_Boundary(twitch_system_boundary, "Twitch System") {
        Container(client, "Client Application", "Desktop, Mobile, Web App", "The user-facing application for streaming and viewing.")
        Container(api_gateway, "API Gateway", "Go/NGINX", "Routes API calls and handles auth.")
        
        Container(ingestion_service, "Ingestion Service", "C++", "Accepts live video streams from streamers.")
        Container(transcoding_service, "Transcoding Service", "Custom/FFmpeg", "Converts streams to multiple resolutions/bitrates.")
        Container(stream_service, "Stream Service", "Java", "Manages stream metadata and state.")
        Container(chat_service, "Chat Service", "Node.js", "Handles real-time chat messages.")
        Container(user_service, "User Service", "Java", "Manages user profiles and accounts.")
        Container(vod_service, "VOD Service", "Python", "Handles playback of recorded streams.")
        Container_Ext(cdn, "CDN Service", "Global Network", "Delivers live and VOD content to viewers.")
        
        ContainerDb(video_blob_store, "Video Blob Store", "AWS S3/GCS", "Stores raw and transcoded video files.")
        ContainerDb(database, "Database", "Cassandra", "Stores user, stream, and chat metadata.")
        Container(message_broker, "Message Broker", "Kafka", "Decouples services and handles event streams.")
    }
    
    Rel(streamer, client, "Uses to broadcast")
    Rel(client, ingestion_service, "Sends video stream to", "RTMP/SRT")
    Rel(client, api_gateway, "Makes API calls to", "HTTPS")
    Rel(viewer, client, "Uses to watch")
    
    Rel(ingestion_service, transcoding_service, "Sends raw stream to", "Async")
    Rel(transcoding_service, cdn, "Publishes transcoded streams to")
    Rel(transcoding_service, video_blob_store, "Archives stream to")
    Rel(stream_service, message_broker, "Publishes stream events to")
    Rel(chat_service, database, "Stores chat history in")
    Rel(chat_service, message_broker, "Publishes chat messages to")
    Rel(vod_service, video_blob_store, "Reads VOD content from")
    Rel(client, cdn, "Fetches live stream from", "HTTP-FLV/HLS/DASH")
```

-----

#### 3\. Component Diagram (Ingestion Service)

This diagram focuses on the internal components of the **Ingestion Service** container, illustrating how it receives and processes a live-stream from a streamer.

**Description:**

  * **API Controller:** The entry point for the Ingestion Service. It receives the stream key and initial connection request.
  * **Authentication & Authorization:** Verifies the streamer's credentials using the provided stream key.
  * **Stream Receiver:** A high-performance component that accepts the raw RTMP or SRT video stream from the streamer.
  * **Media Buffer:** A temporary, in-memory buffer that holds a segment of the incoming stream.
  * **Media Publisher:** A component that publishes the raw stream to the next stage in the pipeline, which is the Transcoding Service, via an internal message queue or stream.
  * **Monitoring Component:** Tracks stream health, bitrate, and latency, reporting metrics to a central monitoring system.

**Mermaid.js Code:**

```mermaid
C4Component
    title Component Diagram for Ingestion Service

    Container(client, "Client Application", "Streamer's PC")
    Container_Ext(transcoding_service, "Transcoding Service")
    Container_Ext(user_service, "User Service", "Auth Service")

    Container_Boundary(ingestion_service, "Ingestion Service") {
        Component(api_controller, "API Controller", "TCP Listener", "Receives stream connection requests.")
        Component(auth_component, "Auth & Auth Component", "Logic Component", "Verifies stream key and user.")
        Component(stream_receiver, "Stream Receiver", "RTMP/SRT Server", "Ingests the raw video stream.")
        Component(media_buffer, "Media Buffer", "In-memory Buffer", "Temporarily stores stream segments.")
        Component(media_publisher, "Media Publisher", "Message Producer", "Sends buffered segments to the transcoder.")
        Component(monitoring, "Monitoring Component", "Metric Reporter", "Monitors stream health and metrics.")
    }
    
    Rel(client, api_controller, "Establishes connection to")
    Rel(api_controller, auth_component, "Sends stream key for verification to")
    Rel(auth_component, user_service, "Queries user credentials from")
    Rel(api_controller, stream_receiver, "Redirects stream to")
    Rel(stream_receiver, media_buffer, "Feeds raw stream segments to")
    Rel(media_buffer, media_publisher, "Pulls segments from")
    Rel(media_publisher, transcoding_service, "Publishes stream to", "Internal Stream")
    Rel(media_buffer, monitoring, "Provides stream metrics to")
```

-----

### Architecture Decision Record

#### ADR 1: Use a Distributed Transcoding Pipeline for Real-Time Adaptability

**Status:** Proposed

**Context:**
A core challenge for a live-streaming platform is delivering a high-quality video stream to a diverse set of devices and network conditions. A streamer uploads a single, high-bitrate stream, but a viewer on a mobile device on a slow network cannot consume this stream. Forcing a single format would lead to buffering and a poor user experience. Therefore, the live stream must be converted into multiple formats, a process known as **transcoding**. This process must be real-time and highly scalable to handle millions of simultaneous streams.

**Decision:**
We will implement a **distributed transcoding pipeline** that operates immediately after a live stream is ingested. The raw stream will be sent to a fleet of distributed transcoding servers. Each server will be responsible for converting the stream into various resolutions and bitrates (e.g., 1080p, 720p, 480p, 360p). These transcoded streams will then be packaged into small, chunked files (e.g., using HLS or MPEG-DASH) and pushed to a global CDN for low-latency delivery.

**Consequences:**

  * **Positive:**
      * **Adaptive Streaming:** Viewers can seamlessly switch between resolutions based on their network conditions and device capabilities, eliminating buffering and ensuring a consistent experience.
      * **Scalability:** The transcoding service can be scaled independently of other services. We can add more transcoding nodes to handle peak streaming events.
      * **Resilience:** The pipeline is designed to be fault-tolerant. If one transcoding node fails, its workload can be redistributed to other nodes.
  * **Negative:**
      * **High Computational Cost:** Transcoding is a CPU-intensive process, requiring a massive amount of computational resources. This is a significant operational cost.
      * **Increased Complexity:** The transcoding pipeline itself is complex to manage, requiring sophisticated logic for load balancing, error handling, and quality control.
      * **Increased Latency:** The transcoding process introduces a small but unavoidable latency to the live stream. While modern techniques can keep this minimal (2-5 seconds), it is a trade-off.

**Alternatives Considered:**

  * **Client-Side Transcoding:** Having the streamer's client application transcode the video into multiple bitrates before sending it. This shifts the computational burden to the streamer but is unreliable, as not all clients have the necessary processing power or consistent network upload speed. This was rejected due to its lack of reliability and dependence on user hardware.
  * **Just-in-Time Transcoding:** Storing the raw stream and transcoding it on-demand as viewers request different qualities. This is not feasible for a live-stream platform, as it would introduce unacceptable latency and require an immense amount of I/O for real-time delivery. This was rejected for its high latency.
