System Design for: 'Zoom'
This document outlines the architectural design for a scalable and real-time video conferencing platform like Zoom. The system is designed to handle a massive volume of concurrent users, provide low-latency audio/video streaming, and manage features like screen sharing, chat, and large-scale webinars, all while ensuring high reliability and quality.

C4 Model Diagrams
1. System Context Diagram
The System Context diagram provides a high-level view of the Zoom system and its interactions with external users and systems. It shows the core platform interacting with meeting participants, external phone systems, and third-party calendar services.

Description:

User: A person who joins or hosts a video conference, either as a participant or a viewer.

Zoom System (System in Scope): The central platform that facilitates all video, audio, and data communication.

External Calendar Service: A third-party system (e.g., Google Calendar, Outlook) used for scheduling meetings and sending invites.

External Phone System: A third-party system that allows users to join meetings via a phone call.

Mermaid.js Code:

C4Context
    title System Context Diagram for Zoom

    Person(user, "User", "Hosts or joins a video conference.")

    System(zoom_system, "Zoom", "The core system for video conferencing.")

    System_Ext(calendar_service, "External Calendar Service", "Schedules meetings and sends invites.")
    System_Ext(phone_system, "External Phone System", "Allows users to join via phone.")

    Rel(user, zoom_system, "Connects to and participates in a meeting on")
    Rel(zoom_system, calendar_service, "Integrates with to schedule meetings")
    Rel(zoom_system, phone_system, "Allows dial-in access via")

2. Container Diagram
This diagram zooms into the Zoom system, showing the major independently deployable components. The design is based on a distributed, microservices architecture with a focus on real-time media processing.

Description:

Client Application (Desktop/Mobile/Web): The user-facing application that captures and renders audio/video, and provides the interface for meeting controls.

API Gateway: The single entry point for all client requests, handling authentication, and routing to various backend services.

Signaling Service: A critical service that manages the connection and state of each meeting. It handles user joins/leaves, chat messages, and participant metadata. It does not handle media data.

Media Ingress/Egress Service: A high-throughput service that handles the secure ingestion of audio/video streams from participants and the delivery of aggregated streams back to them.

Media Router/Mixer: A core component that receives multiple media streams, processes them (e.g., mixing audio, forwarding video), and routes the final streams to the correct participants.

Meeting Service: Manages the lifecycle of a meeting, including creation, scheduling, and security settings.

Chat Service: Handles all in-meeting and persistent chat messages.

User Service: Manages user profiles, authentication, and contact lists.

Media Relay Nodes: A global network of distributed servers that reduce latency by placing media processing closer to users. This is a critical component for quality.

Database: A data store for meeting metadata, user profiles, and chat history.

Load Balancer: Distributes incoming media connections to the nearest available Media Relay Node.

Mermaid.js Code:

C4Container
    title Container Diagram for Zoom

    Person(user, "User")

    System_Boundary(zoom_system_boundary, "Zoom System") {
        Container(client, "Client Application", "Desktop, Mobile, Web App", "Captures and renders audio/video.")
        Container(api_gateway, "API Gateway", "Go/NGINX", "Routes API calls and handles auth.")
        Container(signaling_service, "Signaling Service", "Java", "Manages meeting state and participant metadata.")
        Container(media_ingress_egress, "Media Ingress/Egress Service", "C++", "Handles secure media stream ingestion and delivery.")
        Container(media_router, "Media Router/Mixer", "C++", "Processes and routes media streams.")
        Container(meeting_service, "Meeting Service", "Python", "Manages meeting lifecycle and settings.")
        Container(chat_service, "Chat Service", "Node.js", "Handles meeting and persistent chat.")
        Container(user_service, "User Service", "Java", "Manages user profiles and authentication.")
        Container(media_relay_nodes, "Media Relay Nodes", "C++ Servers", "A global network for media processing.")

        ContainerDb(database, "Database", "Cassandra/PostgreSQL", "Stores user, meeting, and chat metadata.")
        Container(load_balancer, "Load Balancer", "NGINX/HAProxy", "Distributes media connections to relay nodes.")
    }

    Rel(user, client, "Uses")
    Rel(client, api_gateway, "Makes API calls to", "HTTPS")
    Rel(client, load_balancer, "Connects to media streams via")

    Rel(api_gateway, signaling_service, "Routes signaling requests to")
    Rel(api_gateway, meeting_service, "Routes meeting setup requests to")
    Rel(api_gateway, chat_service, "Routes chat requests to")
    Rel(api_gateway, user_service, "Routes auth requests to")

    Rel(load_balancer, media_relay_nodes, "Routes media traffic to")
    Rel(media_relay_nodes, media_ingress_egress, "Sends media streams to")
    Rel(media_ingress_egress, media_router, "Feeds media streams to")
    Rel(media_router, media_ingress_egress, "Sends processed streams back to")
    Rel(media_ingress_egress, client, "Delivers streams to", "UDP/TCP")

3. Component Diagram (Media Router/Mixer)
This diagram focuses on the internal components of the Media Router/Mixer container, illustrating how it handles and processes media streams for a single meeting.

Description:

Ingestion Port: Receives a media stream (audio and/or video) from a participant via the Media Ingress/Egress service.

Audio Processor: A component that handles audio streams. It can perform functions like noise cancellation, echo suppression, and mixing multiple audio streams into one.

Video Processor: A component that handles video streams. It can perform functions like transcoding, resizing, and dynamically selecting which video stream to send to each participant based on the speaker.

Egress Port: Sends the processed and aggregated audio/video streams to the participants via the Media Ingress/Egress service.

Session Manager: A component that maintains the state of the meeting, including which participants are connected, their media stream properties, and the current speaker.

Mermaid.js Code:

C4Component
    title Component Diagram for Media Router/Mixer

    Container(media_ingress_egress, "Media Ingress/Egress Service")

    Container_Boundary(media_router_boundary, "Media Router/Mixer") {
        Component(ingestion_port, "Ingestion Port", "Stream Receiver", "Receives media streams from participants.")
        Component(audio_processor, "Audio Processor", "Logic Component", "Mixes and processes audio streams.")
        Component(video_processor, "Video Processor", "Logic Component", "Transcodes and selects video streams.")
        Component(egress_port, "Egress Port", "Stream Sender", "Sends processed streams to participants.")
        Component(session_manager, "Session Manager", "Stateful Logic", "Manages meeting state and participants.")
    }

    Rel(media_ingress_egress, ingestion_port, "Sends raw media streams to")
    Rel(ingestion_port, audio_processor, "Feeds audio stream to")
    Rel(ingestion_port, video_processor, "Feeds video stream to")
    Rel(ingestion_port, session_manager, "Updates participant status in")
    Rel(audio_processor, egress_port, "Sends processed audio to")
    Rel(video_processor, egress_port, "Sends processed video to")
    Rel(egress_port, media_ingress_egress, "Sends final streams to")
    Rel(session_manager, audio_processor, "Provides routing info to")
    Rel(session_manager, video_processor, "Provides routing info to")

Architecture Decision Records
ADR 1: Use a Mesh vs. SFU vs. MCU Architecture for Media Routing
Status: Proposed

Context:
A key design decision for a real-time video conferencing system is how to route media streams between participants. The choice of architecture directly impacts latency, server load, and video quality. There are three primary models:

Mesh: Each participant sends their video to every other participant. This is simple but doesn't scale well beyond a few users due to massive bandwidth consumption on the client side.

MCU (Multipoint Control Unit): All streams are sent to a central server, which mixes them into a single stream and sends it back to each participant. This is computationally intensive for the server and introduces latency.

SFU (Selective Forwarding Unit): Each participant sends their video to a central server, which then forwards the individual streams to other participants without mixing them. This is the model used by most modern, scalable systems.

Decision:
We will implement an SFU (Selective Forwarding Unit) architecture. Each participant will upload their audio and video streams to a designated Media Relay Node (the SFU). The SFU will then intelligently forward the individual streams of other participants to each client. This architecture provides the best balance of scalability, performance, and flexibility.

Consequences:

Positive:

Scalability: The SFU scales much better than the Mesh model, as client bandwidth consumption is limited to the number of participants displayed on the screen.

Reduced Server Load: Unlike the MCU, the SFU does not mix streams, which offloads a significant amount of CPU-intensive work to the clients.

Flexibility: The SFU can selectively choose which streams to forward (e.g., only the current speaker's video), optimizing bandwidth for clients with limited connectivity. This is a key feature for a responsive user experience.

Negative:

Increased Complexity: The SFU architecture is more complex to implement than a simple Mesh. It requires intelligent routing, stream prioritization, and robust handling of network jitter and packet loss.

Single Point of Failure: While distributed, if a single SFU server fails, all participants connected to it will be disconnected. This requires a robust failover mechanism.

Latency: While better than an MCU, there is still some latency introduced by routing through the SFU compared to a peer-to-peer (Mesh) connection.

Alternatives Considered:

Mesh Architecture: This model would be simple to implement for small meetings but would be unusable for meetings with more than a handful of participants due to the exponential increase in bandwidth required from each client. It was rejected because the system must be highly scalable.

MCU (Multipoint Control Unit) Architecture: This model is simple for clients, as they only receive one stream, but it is not scalable for the server. The computational resources required to mix hundreds or thousands of streams would be immense, making it prohibitively expensive and a significant bottleneck. It was rejected for scalability and cost reasons.

ADR 2: Real-time Data Transport Protocol for Media
Status: Accepted

Context:
For real-time audio and video streaming, selecting the appropriate transport protocol is critical to ensure low latency, high throughput, and resilience against network conditions. The primary choices are TCP (Transmission Control Protocol) and UDP (User Datagram Protocol), each with distinct characteristics impacting real-time media.

TCP: Provides reliable, ordered, and error-checked delivery of data. It retransmits lost packets and controls flow, ensuring data integrity.

UDP: A connectionless protocol that offers fast, unreliable data delivery. It does not guarantee delivery, order, or error checking, making it suitable for applications where timeliness is more critical than perfect reliability.

Decision:
We will primarily utilize UDP for real-time audio and video media streams, specifically leveraging RTP (Real-time Transport Protocol) over UDP. Signaling and control plane traffic will continue to use TCP/HTTPS for reliability and security. For situations where UDP is blocked or highly restricted (e.g., corporate firewalls), a fallback mechanism will be implemented to tunnel media over TCP/TLS, albeit with an expected increase in latency.

Consequences:

Positive:

Low Latency: UDP's connectionless nature and lack of retransmission overhead significantly reduce latency, which is paramount for real-time interactive communication.

Jitter Resilience: RTP over UDP includes mechanisms (e.g., sequence numbers, timestamps) that allow for jitter buffering and out-of-order packet handling, improving the user experience under fluctuating network conditions.

Efficient Bandwidth Usage: No retransmissions means less wasted bandwidth compared to TCP when packet loss occurs, as retransmitting old frames in a real-time stream is often less valuable than receiving new ones.

Negative:

Unreliable Delivery: UDP offers no guarantees of packet delivery, which means some audio/video frames may be lost. This needs to be managed at the application layer through error concealment techniques.

NAT/Firewall Traversal Complexity: UDP traffic can be more challenging to traverse Network Address Translators (NATs) and firewalls compared to TCP, requiring sophisticated techniques like STUN/TURN/ICE.

Congestion Control: UDP lacks inherent congestion control mechanisms, requiring application-level logic (e.g., bandwidth estimation, adaptive bitrate streaming) to prevent network saturation.

Alternatives Considered:

TCP for Media Streams: While simpler for NAT/firewall traversal and offering guaranteed delivery, TCP's retransmission mechanisms and head-of-line blocking would introduce unacceptable latency and jitter for real-time video conferencing, especially under packet loss. Retransmitting a video frame that is already several milliseconds old is often detrimental to user perception.

WebRTC Data Channels for Media: While WebRTC data channels can use SCTP over UDP for more reliable, ordered, but still low-latency data, they are generally intended for arbitrary data rather than optimized for real-time audio/video. RTP over UDP is the established and highly optimized standard for this use case.

ADR 3: Data Store for Meeting Metadata and Chat History
Status: Accepted

Context:
The system needs to store various types of data, including user profiles, meeting configurations (scheduled time, participants, security settings), and the history of in-meeting chats. Given the potentially massive scale of users and meetings, the data store must be highly available, scalable, performant for both reads and writes, and capable of handling diverse data structures.

Relational Databases (e.g., PostgreSQL): Offer strong consistency (ACID properties), structured schemas, and powerful querying capabilities with JOINs.

NoSQL Databases (e.g., Cassandra, MongoDB): Provide high availability, horizontal scalability, and flexible schemas, often at the expense of strict consistency and complex querying across different data types.

Decision:
We will implement a polyglot persistence strategy, utilizing both PostgreSQL for core meeting metadata and user profiles and Cassandra for chat history.

PostgreSQL: Will store structured data like user accounts, meeting schedules, participant lists, and meeting settings. This data requires strong consistency and transactional integrity.

Cassandra: Will store the high-volume, time-series chat messages. Chat history benefits from Cassandra's high write throughput, eventual consistency model, and horizontal scalability across many nodes.

Consequences:

Positive:

Optimized Performance: Each database is chosen for its strengths, providing optimal read/write performance for specific data types. PostgreSQL excels at transactional integrity and complex queries for structured data, while Cassandra is ideal for high-volume, append-only data like chat.

Scalability: Cassandra provides excellent horizontal scalability for chat data, handling the potentially massive volume without becoming a bottleneck. PostgreSQL can be scaled vertically and horizontally (e.g., read replicas, sharding) for meeting metadata.

Data Integrity: Strong consistency for critical metadata (meetings, users) is maintained by PostgreSQL.

Negative:

Increased Operational Complexity: Managing and maintaining two different database systems (PostgreSQL and Cassandra) adds overhead in terms of deployment, monitoring, backup, and expertise.

Data Consistency Challenges: While strong consistency is maintained within each store for its respective data, ensuring cross-system consistency for any shared concepts (though minimized here) requires careful design.

Potential for Data Silos: Developers need to be aware of which data resides where and avoid joining across different database types in application code, which could lead to performance issues or data integrity problems if not carefully managed.

Alternatives Considered:

Solely PostgreSQL: While robust, using PostgreSQL for all data, especially chat history, would eventually hit scalability limits for write throughput and storage capacity for a system expecting millions of concurrent users. Sharding would be necessary but would add significant complexity.

Solely Cassandra: Using Cassandra for all data would introduce challenges for transactional integrity and complex relational queries needed for user management and sophisticated meeting configurations. While flexible, schema management and querying for highly structured data would be less efficient than with a relational database.

MongoDB: Could be considered for its document-oriented model. However, for real-time chat, Cassandra's distributed nature and write-optimized architecture are generally superior for extremely high-volume, append-only data. For highly relational meeting metadata, PostgreSQL still provides stronger guarantees and more mature features.

ADR 4: Authentication and Authorization Mechanism
Status: Proposed

Context:
A robust authentication and authorization system is fundamental to securing user access, meeting rooms, and data within the Zoom platform. The system needs to support user login, session management, and granular control over permissions (e.g., host vs. participant, screen sharing rights). Key considerations include security, scalability, ease of integration, and user experience.

Custom Token-Based Authentication: Developing a proprietary system where the service issues and validates its own tokens (e.g., JWTs) after user credentials are verified.

OAuth 2.0 / OpenID Connect (OIDC): Leveraging industry-standard protocols for delegated authorization and authentication, often integrated with Identity Providers (IdPs) like Google, Microsoft, or Okta.

Decision:
We will implement an OAuth 2.0 flow with OpenID Connect (OIDC) as the primary authentication and authorization mechanism. This will involve an internal Identity Provider (IdP) service that handles user registration, login, and issues ID tokens (OIDC) and access tokens (OAuth 2.0). These tokens will then be used by clients to authenticate with the API Gateway and by backend services to authorize requests. The system will also support integration with external IdPs for enterprise clients (e.g., SAML, Azure AD).

Consequences:

Positive:

Enhanced Security: Leveraging well-vetted, industry-standard protocols significantly reduces the risk of security vulnerabilities compared to a custom implementation. OIDC provides strong guarantees for user identity verification.

Scalability & Performance: Token-based authentication (like JWTs issued by OIDC) is stateless on the server side (after initial verification), allowing the API Gateway and backend services to scale horizontally without needing to maintain session state for each authenticated user.

Interoperability: Standard protocols make it easier to integrate with third-party applications and enterprise identity systems (e.g., single sign-on).

Clear Separation of Concerns: Authentication and authorization logic are centralized within the IdP service, simplifying development of other microservices.

Negative:

Increased Initial Complexity: Implementing and managing an OIDC-compliant IdP, even an internal one, is more complex than a basic custom token system. It requires understanding intricate protocol flows and security best practices.

Token Management: Proper management of token lifetimes, revocation, and refresh tokens is critical to security and user experience.

Dependency on IdP: The entire system relies on the availability and performance of the IdP service for user authentication.

Alternatives Considered:

Custom Token-Based Authentication: While offering full control, developing a custom token system from scratch is risky. It's prone to security flaws if not implemented by experts, requires continuous maintenance to keep up with evolving threats, and lacks the immediate interoperability benefits of standards. This was rejected due to the high security requirements of a video conferencing platform.

Session-Based Authentication (Cookies): This model would involve servers maintaining session state, which would introduce significant challenges for horizontal scalability, especially across distributed microservices. Sticky sessions would be required, complicating load balancing and failover. This was rejected for scalability reasons.
